import { columnIndexOf, columnExtents } from '../utils/datatable-utils';
import aggregatorStore from '../aggregators/index.js';
import { getConfig } from '../globals/global-config';
import { OperatorTypes, FieldType } from '../enums';
import { leftMostExactOrGreater } from '../utils/sorted-search';
import { numberComparator } from '../toolbox/src/index';
import { getDateStart, duration, interval } from '../time-utils/src/index';

const EMPTY_STR = '';
/**
 * Performs group by operation on current data table.
 *
 * @param {array} groupConfigArr - The group configuration array
 * @param {array} aggrConfigArr - The aggregation configuration array
 * @returns {array} - grouped data rows
 */
function groupBy (groupConfigArr, aggrConfigArr) {
  let groupConfigArray = groupConfigArr, aggrConfigArray = aggrConfigArr;
  return {
    ops: 'groupBy',
    type: OperatorTypes.GroupBy,
    _updateArgs: (groupConfiguration, aggrConfiguration) => {
      groupConfigArray = groupConfiguration;
      aggrConfigArray = aggrConfiguration;
    },
    fn: (_data, schema, config, generatorFn) => {
      const data = typeof generatorFn === 'function' ? generatorFn() : _data;

      if (!groupConfigArray || groupConfigArray.length === 0) throw new Error('groupConfigArray cannot be empty');
      if (!aggrConfigArray || aggrConfigArray.length === 0) throw new Error('aggrConfigArray cannot be empty');

      if (!(groupConfigArray instanceof Array) && groupConfigArray instanceof Object) {
        groupConfigArray = [groupConfigArray];
      }
      if (!(aggrConfigArray instanceof Array) && aggrConfigArray instanceof Object) {
        aggrConfigArray = [aggrConfigArray];
      }

      if (groupConfigArray.length === 1 && config && groupConfigArray[0].column === config.indexBy) {
        // if single and indexBy column
        return _singleSortedGroupBy(data, schema, config, groupConfigArray[0], aggrConfigArray);
      }
      // OR, unsorted single/multiple columns
      return _genericGroupBy(data, schema, config, groupConfigArray, aggrConfigArray);
    }
  };
}

/**
 * Takes both the group config and aggregation config arrays,
 * and creates and returns a schema from the two.
 *
 * @param {array} schema The schema
 * @param {array} _groupConfig The group configuration array
 * @param {array} _aggrConfig The aggregation configuration array
 */
function _createAggrSchema (schema, _groupConfig, _aggrConfig, cacheColumnConfig) {
  var newSchema = [], groupConfig = _groupConfig, aggrConfig = _aggrConfig, groupConfigLength = groupConfig.length, aggrConfigLength = aggrConfig.length, column, type, aggregatorFn, i, operation, newColumn;

  groupConfig = groupConfig instanceof Array ? groupConfig : [groupConfig];
  aggrConfig = aggrConfig instanceof Array ? aggrConfig : [aggrConfig];

  // add the group columns in new schema
  for (i = 0; i < groupConfigLength; i++) {
    column = cacheColumnConfig(groupConfig[i].column, schema);

    // update the schema
    newColumn = {
      name: groupConfig[i].outputAs || groupConfig[i].column,
      type: column.type === FieldType.DateTime ? FieldType.Interval : column.type
    };
    if (typeof column.enableUTC !== 'undefined') {
      newColumn.enableUTC = column.enableUTC;
    }
    newSchema.push(newColumn);
  }

  for (i = 0; i < aggrConfigLength; i++) {
    // default operation is avg
    operation = aggrConfig[i].operation || 'avg';
    column = cacheColumnConfig(aggrConfig[i].column, schema);

    aggregatorFn = aggregatorStore.resolve(operation);
    if (!aggregatorFn) throw new Error(`${operation} is not a defined operation`);

    // if operation is first or last then inherit from schema's column
    // if it's count, type will be number
    // rest are all numeric operations
    // however, if the column type is not numeric, throw an error
    switch (operation) {
      case 'first':
      case 'last':
        type = column.type;
        break;
      case 'count':
        type = FieldType.Number;
        break;
      default:
        if (column.type !== FieldType.Number) throw new Error(`${operation} can apply only on numbers`);
        type = FieldType.Number;
    }

    // update the schema
    newSchema.push({
      name: aggrConfig[i].outputAs || (aggrConfig[i].column + (operation ? ' - ' + operation : '')),
      type: type
    });
  }

  return newSchema;
}

/**
 * Performs group by on a single column.
 *
 * @param {array} data The datatable being operated on
 * @param {array} schema The schema
 * @param {array} groupConfig The group configuration array
 * @param {array} aggrConfig The aggregation configuration array
 */
function _singleSortedGroupBy (data, schema, config, groupConfig, aggrConfig) {
  var newData = [],
    newSchema = [],
    dataLength = data.length,
    column,
    maxDate,
    timeDuration,
    startValFloorVal,
    startPos = 0,
    lastEntry,
    cacheColumnConfig = _cacheColumnConfig(),
    cacheGroupCount = [],
    newDataLength,
    enableUTC,
    i;

  column = cacheColumnConfig(groupConfig.column, schema);
  column.outputAs = groupConfig.outputAs;

  // prepare the schema
  newSchema = _createAggrSchema(schema, [column], aggrConfig, cacheColumnConfig);

  if (dataLength > 0) {
    enableUTC = column.enableUTC || getConfig('enableUTC');
    // if it's a date column,
    //  create a duration object
    //  if startValue is present/number, else use first row's datetime
    //    get the start value of the dataset
    if (column.type === FieldType.DateTime) {
      let peaks, minDate, startValue;
      timeDuration = duration(groupConfig.timeUnit, Math.abs(groupConfig.binSize) || 1);
      if (!data[0] || !data[0][column.index] || !data[dataLength - 1] || !data[dataLength - 1][column.index]) {
        peaks = columnExtents(groupConfig.column, data, schema, config.indexBy);
      }
      minDate = (data[0] && data[0][column.index]) || peaks.min;
      startValue = groupConfig.startValue && parseInt(groupConfig.startValue, 10);
      if (startValue) {
        if (startValue >= minDate) {
          let comparer = (a, b) => numberComparator(a[column.index], b);
          // if start value falls on or after the minimum date in dataset, then floor it to unit with binning 1
          startValFloorVal = getDateStart(startValue, duration(groupConfig.timeUnit, 1), enableUTC, groupConfig.weekStartFrom);
          // get the index of left most occurrence
          startPos = leftMostExactOrGreater(startValFloorVal, data, comparer, 0, data.length);
        } else {
          // if start value falls before the minimum date in dataset, then floor the minimum to date unit with binning binSize
          startValFloorVal = getDateStart(minDate, timeDuration, enableUTC, groupConfig.weekStartFrom, startValue);
        }
      } else {
        // if start value absent, then floor the minimum to date unit with binning 1
        startValFloorVal = getDateStart(minDate, duration(groupConfig.timeUnit, 1), enableUTC, groupConfig.weekStartFrom);
      }
      maxDate = (data[dataLength - 1] && data[dataLength - 1][column.index]) || peaks.max;
    }

    // if start value is less than maxdate only then proceed
    if (typeof startValFloorVal === 'undefined' || startValFloorVal <= maxDate) {
      let intervalConfig = {
        duration: timeDuration,
        outputFormat: groupConfig.outputFormat,
        enableUTC: enableUTC,
        weekStartFrom: groupConfig.weekStartFrom
      };
      if (typeof startValFloorVal !== 'undefined') {
        // if no entry in new data array, create new interval entry,
        // add it to new data array and return it
        lastEntry = [interval(startValFloorVal, intervalConfig)];
        newData.push(lastEntry);
      }
      for (i = startPos; i < dataLength; i++) {
        if (data[i].length > 0) {
          // date field
          // OR, string field OR, number field with binning 1
          if (column.type === FieldType.DateTime) {
            lastEntry = _singleSortedAddRowDate(newData, lastEntry, data[i][column.index], intervalConfig, startValFloorVal);
          } else if (column.type === FieldType.String || column.type === FieldType.Number) {
            lastEntry = _singleSortedAddRow(newData, lastEntry, data[i][column.index]);
          }

          newDataLength = newData.length - 1;

          // init the cacheGroupCount current row with an empty object before aggregation
          cacheGroupCount[newDataLength] = cacheGroupCount[newDataLength] || {};
          lastEntry && _applyAggr(data[i], schema, lastEntry, 1, aggrConfig, cacheColumnConfig, cacheGroupCount[newDataLength]);
        }
      }
    }
  }

  return {
    data: newData,
    schema: newSchema,
    config: { indexBy: (groupConfig.outputAs || groupConfig.column) }
  };
}

/**
 * Performs row addition when group by is on single sorted non-date data.
 *
 * @param {array} newData The new data array after group by
 * @param {array} _lastEntry The last entry in the new data array
 * @param {any} value String or number, the data value
 */
function _singleSortedAddRow (newData, _lastEntry, value) {
  let lastEntry = _lastEntry;
  // if no entry in new data array OR, the current value doesn't fit in the last entry,
  // make a new entry in the new data array and return it
  if (!lastEntry || value !== lastEntry[0]) {
    lastEntry = [value];
    newData.push(lastEntry);
  }
  return lastEntry;
}

/**
 * Performs row addition when group by is on single sorted date data.
 *
 * @param {array} newData The new data array after group by
 * @param {array} _lastEntry The last entry in the new data array
 * @param {number} start The start datetime of the interval
 * @param {object} intervalConfig The configuration object used by interval
 * @param {number} minStart The starting datetime of the dataset grouping
 */
function _singleSortedAddRowDate (newData, _lastEntry, start, intervalConfig, minStart) {
  let lastEntry = _lastEntry;
  // if current date is greater than or equal to last interval's end
  // then create a new interval entry starting from the previous interval's end,
  // add it to the new data array, and repeat

  if (start >= lastEntry[0].end) {
    let startValFloorVal;
    switch (intervalConfig.duration.Unit) {
      case 'Month':
      case 'Quarter':
      case 'Year':
        startValFloorVal = getDateStart(start, intervalConfig.duration, intervalConfig.enableUTC, intervalConfig.weekStartFrom, minStart);
        break;

      default:
        startValFloorVal = start - ((start - minStart) % (intervalConfig.duration.ms));
        break;
    }
    lastEntry = [interval(startValFloorVal, intervalConfig)];
    newData.push(lastEntry);
  }
  return lastEntry;
}

/**
 * Caches the column's index in the original data schema.
 */
function _cacheColumnConfig () {
  let cache = {};
  return function (columnName, schema, data) {
    if (!cache[columnName]) {
      let columnIndex = columnIndexOf(columnName, schema),
        column = schema[columnIndex];

      if (typeof column === 'undefined') throw new Error('incorrect column name in config - ' + columnName);

      cache[columnName] = column;
      cache[columnName].column = columnName;
      cache[columnName].index = columnIndex;

      if (column.type === FieldType.DateTime && data) {
        cache[columnName].peaks = columnExtents(columnName, data, schema);
      }
    }
    return cache[columnName];
  };
}

/**
 * Performs group by on unsorted/multiple column data.
 *
 * @param {array} data The datatable being operated on
 * @param {array} schema The schema
 * @param {array} groupConfig The group configuration array
 * @param {array} aggrConfig The aggregation configuration array
 */
function _genericGroupBy (data, schema, config, groupConfig, aggrConfig) {
  var newData = [],
    newSchema = [],
    groupConfigCopy = [],
    loopData = true,
    dataLength = data.length,
    groupConfigLength = groupConfig.length,
    column,
    startValFloorVal,
    dateColumns = {},
    cacheGroupCount = [],
    cacheColumnConfig = _cacheColumnConfig(),
    hashTable = {},
    hashCount = 0,
    indexBy,
    i;

  // prepare the schema
  // loop through the group config
  for (i = 0; i < groupConfigLength; i++) {
    // get column details
    column = cacheColumnConfig(groupConfig[i].column, schema, data);
    column.outputAs = groupConfig[i].outputAs;

    // if date, store in separate array
    //  else, add to row
    // in both cases create the hash value
    if (column.type === FieldType.DateTime) {
      if (!dateColumns[i]) {
        // if group config start value exceeds max of column, then prepare for exiting data processing
        if (groupConfig[i].startValue >= column.peaks.max) {
          loopData = false;
        }

        dateColumns[i] = Object.assign({}, column);
        dateColumns[i].intervalConfig = {
          duration: duration(groupConfig[i].timeUnit, Math.abs(groupConfig[i].binSize) || 1),
          outputFormat: groupConfig[i].outputFormat,
          enableUTC: column.enableUTC || getConfig('enableUTC'),
          weekStartFrom: groupConfig[i].weekStartFrom
        };
        startValFloorVal = parseInt(groupConfig[i].startValue, 10) || column.peaks.min;
        dateColumns[i].startValFloorVal = startValFloorVal && getDateStart(startValFloorVal, duration(groupConfig[i].timeUnit, 1), dateColumns[i].intervalConfig.enableUTC, dateColumns[i].intervalConfig.weekStartFrom, parseInt(groupConfig[i].startValue, 10));
        dateColumns[i].max = column.peaks.max;
      }
    }
    groupConfigCopy[i] = column;

    // process the indexby
    if (!indexBy && groupConfig[i].column === config.indexBy) {
      indexBy = groupConfig[i].outputAs || groupConfig[i].column;
    }
  }

  newSchema = _createAggrSchema(schema, groupConfigCopy, aggrConfig, cacheColumnConfig);

  // loop through the data
  if (loopData && dataLength > 0) {
    for (i = 0; i < dataLength; i++) {
      hashCount = _genericGroupByProcessRow(data[i], schema, groupConfig, aggrConfig, cacheColumnConfig, dateColumns, hashTable, hashCount, newData, cacheGroupCount);
    }
  }

  return {
    data: newData,
    schema: newSchema,
    config: { indexBy: indexBy }
  };
}

/**
 * Processes a data row and makes an entry/aggregation in the grouped data.
 *
 * @param {array} dataRow The datatable row being operated on
 * @param {array} schema The schema
 * @param {array} groupConfig The group configuration array
 * @param {array} aggrConfig The aggregation configuration array
 * @param {function} cacheColumnConfig  The cache storage for column configs
 * @param {object} dateColumns Stores column configs of all date columns
 * @param {object} hashTable Stores the hasvalues for data rows
 * @param {number} _hashCount The current hashcount in the hashtable
 * @param {array} newData The new data array after group by
 * @param {array} cacheGroupCount
 */
function _genericGroupByProcessRow (dataRow, schema, groupConfig, aggrConfig, cacheColumnConfig, dateColumns, hashTable, _hashCount, newData, cacheGroupCount) {
  let j, column, hashVal = EMPTY_STR, currentRow = [], cacheGroupCountIndex, intervalObj, groupConfigLength = groupConfig.length, hashCount = _hashCount;
  if (dataRow.length > 0) {
    // loop through the group config
    for (j = 0; j < groupConfigLength; j++) {
      // get column details
      column = cacheColumnConfig(groupConfig[j].column, schema);

      // if date, store the startValue
      //  else, add to row
      // in both cases create the hash value
      if (column.type === FieldType.DateTime) {
        // if date is less than start value then exclude the row
        if (dateColumns[j].startValFloorVal > dataRow[column.index]) {
          return hashCount;
        }

        // always calculate the rounded date given the start of the column data and binsize
        switch (groupConfig[j].timeUnit.name) {
          case 'Month':
          case 'Quarter':
          case 'Year':
            dateColumns[j].value = getDateStart(dataRow[column.index], dateColumns[j].intervalConfig.duration, dateColumns[j].intervalConfig.enableUTC, dateColumns[j].intervalConfig.weekStartFrom, dateColumns[j].startValFloorVal);
            break;

          default:
            dateColumns[j].value = dataRow[column.index] - ((dataRow[column.index] - dateColumns[j].startValFloorVal) % (dateColumns[j].intervalConfig.duration.ms));
            break;
        }
        hashVal += dateColumns[j].value + '-';
      } else {
        currentRow[j] = dataRow[column.index];
        hashVal += dataRow[column.index] + '-';
      }
    }

    // remove the last '-'
    hashVal = hashVal.slice(0, -1);

    // search hash value in hash table
    if (hashTable[hashVal] >= 0) {
      // get the row from the grouped data array
      currentRow = newData[hashTable[hashVal]];

      cacheGroupCountIndex = hashTable[hashVal];
    } else {
      // make entry in hash table array
      hashTable[hashVal] = hashCount++;

      // if date storage array present, loop through it
      for (const key in dateColumns) {
        // create interval using derived start date, and other datetime info
        // add interval to grouped data array in the relevant index
        intervalObj = interval(dateColumns[key].value, dateColumns[key].intervalConfig);
        currentRow[key] = intervalObj;
      }
      newData.push(currentRow);

      cacheGroupCountIndex = newData.length - 1;
    }

    // init the cacheGroupCount current row with an empty object before aggregation
    cacheGroupCount[cacheGroupCountIndex] = cacheGroupCount[cacheGroupCountIndex] || {};

    // apply aggregation to this row
    currentRow && _applyAggr(dataRow, schema, currentRow, groupConfigLength, aggrConfig, cacheColumnConfig, cacheGroupCount[cacheGroupCountIndex]);

    return hashCount;
  }
}

/**
 * Perform aggregation operation on given aggregation configuration and a data row.
 *
 * @param {array} dataRow The datatable row being operated on
 * @param {array} schema The schema
 * @param {array} lastEntry The last entry in the new data array
 * @param {number} entryIndex The entry index for the aggregated columns in the row, after the group columns
 * @param {array} aggrConfig The aggregation configuration array
 * @param {function} cacheColumnIndex The reference to the caching function for column indices
 * @param {object} cacheGroupCountRow The current row in the cache group count column
 */
function _applyAggr (dataRow, schema, lastEntry, entryIndex, aggrConfig, cacheColumnConfig, cacheGroupCountRow) {
  let aggrConfigLength = aggrConfig.length, column, operation, count, i;

  for (i = 0; i < aggrConfigLength; i++) {
    column = cacheColumnConfig(aggrConfig[i].column, schema);
    if (typeof dataRow[column.index] !== 'undefined' && dataRow[column.index] !== null) {
      operation = aggrConfig[i].operation || 'avg';

      // for the below operations, calculate and store the current count of the column values in the group, in the cacheGroupCount and count
      switch (operation) {
        case 'sum':
        case 'min':
        case 'max':
        case 'first':
        case 'last':
          count = null;
          break;

        default:
          count = cacheGroupCountRow[i] = (cacheGroupCountRow[i] && (cacheGroupCountRow[i] + 1)) || 1;
          break;
      }

      // for operation other than count, calculate the aggregated value and add to the last entered row
      lastEntry[entryIndex + i] = operation === 'count' ? count : aggregatorStore.resolve(operation)(typeof lastEntry[entryIndex + i] !== 'undefined' ? lastEntry[entryIndex + i] : null, dataRow[column.index], count);
    }
  }
}

export default groupBy;

export { _singleSortedAddRow, _singleSortedAddRowDate };
